{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free Reinforcement Learning: Deep Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will program a model-free reinforcement learning algorithm with a deep Q-network. The tasks will be:\n",
    "\n",
    "    Design the replay buffer and pre-fill the buffer\n",
    "    Design the Q-network\n",
    "    Implement epsilon-greedy\n",
    "    Implement a method for calculating the loss\n",
    "    Put everything together in the trainin loop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all neccessary modules\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import copy, deepcopy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement a replay buffer to break high correlation between similiar trajectories and neighboring data points. In Task 1.1, you need to initialize the buffer meaning you need to store the maximum memory size, a memory counter and arrays for storing the state, action, reward, next_state, and flags for terminal states. In Task 1.2, you implement to store transitions in the arrrays of the buffer. Last, you need to design a method for sampling randomly from the buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    # Task 1.1: Initialization of the replay buffer\n",
    "    # Store the max_size of the buffer and create a counter (int) needed for iterating over the whole buffer when\n",
    "    # storing transitions\n",
    "    # Use numpy arrays! Think about the shape of these arrays!\n",
    "    # The memories should have the following data types:\n",
    "    # states: float32\n",
    "    # next_states: float32\n",
    "    # actions: int64\n",
    "    # rewards: float32\n",
    "    # terminal: bool\n",
    "    def __init__(self, max_size, input_shape):\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "    \n",
    "    # Task 1.2: Store transitions\n",
    "    # Compute the index where you want to store the new transition (Hint: modulo-devision)\n",
    "    # Store the data of the transition at the appropriate position\n",
    "    def append(self, state, action, reward, state_, done):\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "    #Task 1.3: Sample a random batch from the buffer\n",
    "    # Return numpy arrays for states, actions, rewards, next_states and terminal flags\n",
    "    def sample_batch(self, batch_size):\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instantiate a replay buffer with a maximum size of 10000 and fill the buffer with transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.4: Initialize replay buffer\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "pass  # to be replaced by your code\n",
    "\n",
    "######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "\n",
    "# Task 1.5: Fill replay memory\n",
    "# Define variables for the number of actions and states\n",
    "# Generate transitions and fill up the buffer\n",
    "# First, reset the environment\n",
    "# Use a loop to generate a trajectory.\n",
    "# If you encountered a terminal state, leave the loop and start the next trajectory until the buffer is full. \n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "pass  # to be replaced by your code\n",
    "\n",
    "memory_filling_steps = 0 #counter for how many transitions have already been stored\n",
    "while memory_filling_steps < buffer.mem_size:\n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the deep Q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, you need to implement a neural net class for the Q-network. The neural net should have the following architecture:\n",
    "\n",
    "    fully connected: #input_dim -> #hidden_dim\n",
    "    fully connected: #hidden_dim -> #hidden_dim\n",
    "    fully connected: #hidden_dim -> #hidden_dim\n",
    "    fully connected: #hidden_dim -> #n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1: Implement the neural network\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self,  n_actions, input_dims, hidden_dim, bias):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "    def forward(self, state):\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and additional stuff\n",
    "# You don't have to do anything here\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = 4\n",
    "N_HIDDEN_NODES = 256\n",
    "USE_BIAS = True\n",
    "EPSILON = 0.05\n",
    "BATCH_SIZE = 64\n",
    "WINDOW = 100\n",
    "REWARD_THRESHOLD = 195\n",
    "NETWORK_UPDATE_FREQUENCY = 4\n",
    "NETWORK_SYNC_FREQUENCY = 2000\n",
    "DEVICE='cpu'\n",
    "GAMMA = 0.99\n",
    "MAX_EPISODES = 10000\n",
    "\n",
    "training_rewards = []\n",
    "training_loss =[]\n",
    "update_loss = []\n",
    "mean_training_rewards = []\n",
    "sync_eps = []\n",
    "step_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initialize a deep Q-network. Additionaly, you need a target Q-network to stabilize learning. Remember the target network is frequently updated with the weights of the Q-net, so copy the weights of the Q-net into the target right at the beginning. Moreover, instantiate an Adam-optimizer for updating the weights of the Q-net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.2: Instantiate the Q-net, the target net, and the Adam optimizer for the Q-net\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "pass  # to be replaced by your code\n",
    "\n",
    "######################################## END OF YOUR CODE ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon-greedy exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to implement a method for the epsilon-greedy exploration. Epsilon-greedy is a very simple yet effective exploration scheme. If a randomly drawn number between 0 and 1 is less than the epsilon threshold, a random number is chosen, else choose an action according to the greedy policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1: Epsilon-greedy.\n",
    "# First, generate a random number and check if you choose the action randomly or with respect to the greedy policy\n",
    "def get_action(state, epsilon=0.05):\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "# Define a method for computing the action according to the greedy policy\n",
    "# You have to use the Q-net. Therefore, transform the state (numpy state) to a torch tensor and make sure it is \n",
    "# available on your computing device (.to())\n",
    "def greedy_action(state):\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the loss ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where all magic happens. Compute the mean-squarred error loss of the target Q-values and the Q-values according to the following alogrithm (Hasselt et al. 2015). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&for~each~update~step~do\\\\\n",
    "&\\quad sample ~ e_t=(s_t, a_t, r_t, s_t')\\sim D\\\\\n",
    "&\\quad Compute~target~Q~value:\\\\\n",
    "&\\quad \\quad Q^\\star(s_t, a_t) \\approx r_t + \\gamma Q_\\theta(s_{t+1}, argmax_{a'}Q_{\\theta'}(s_{t+}, a'))\\\\\n",
    "&\\quad Perform~gradient~descent~with~MSE~loss~on~(Q^\\star(s_t,a_t)-Q_\\theta(s_t,a_t))^2\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.1: the loss function\n",
    "# First, you need to transform the numpy arrays for the state batch, action batch, reward batch, next state batch\n",
    "# terminal flag batch into appropriate tensors which are accessable from your computing device.\n",
    "# Compute the Q-values for the current state and action.\n",
    "# Compute the the best action under the Q-net for the next state\n",
    "# Compute the target Q-values for the next state and the best action for the next state\n",
    "# Zero-out all target Q-values for terminal states\n",
    "# Compute the expected target Q-values\n",
    "# Compute the loss for between Q-values and expected target Q-values\n",
    "def calculate_loss(state_batch, action_batch, reward_batch, new_state_batch, done_batch):\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nothing to do here\n",
    "def check_average_reward():\n",
    "    val_env = gym.make('CartPole-v0')\n",
    "    policy = deepcopy(q_network)\n",
    "    policy.eval()\n",
    "    trajectory_rewards = np.empty((10, 1))\n",
    "    for i_traj in range(10):\n",
    "        state, ep_reward = val_env.reset(), 0\n",
    "        cum_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.FloatTensor(state).to(device=DEVICE)\n",
    "                a = policy.forward(state_t)\n",
    "                argmax_a = torch.max(a, dim=0)[1].item()\n",
    "            state, reward, done, _ = val_env.step(argmax_a)\n",
    "            cum_reward += reward\n",
    "            if done:\n",
    "                trajectory_rewards[i_traj] = cum_reward\n",
    "                break\n",
    "    average_reward = np.mean(trajectory_rewards)\n",
    "    print(' avg_reward: ', average_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, you need to put everything together in a training loop. Setup the environment, choose the action according to epsilon-greedy. Take a step and add the transition to the buffer. Every 4 steps, we want to perform an update step on the Q-net and every 2000 steps, we want to synchronize the target Q-net with the Q-net. You have already written all code to train the network, just set it up in an appropriate order.\n",
    "\n",
    "Visualize your learning performance using a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.1: the training loop\n",
    "# choose the action according to epsilon-greedy\n",
    "# perform the action \n",
    "# store the transition in the replay buffer\n",
    "# Every 4 steps perform an update step on the Q-net. Don't forget to zero the gradients of the optimizer.\n",
    "#     - Sample a batch from the buffer\n",
    "#     - compute the loss for the batch\n",
    "#     - perform the backpropagation through the Q-net\n",
    "#     - perform an update step on the Q-net using the optimizer\n",
    "# Every 2000 steps synchronize the target Q-net and the current Q-net\n",
    "ep = 0\n",
    "training = True\n",
    "while training:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = 0\n",
    "    while done == False:\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "        if done:\n",
    "            ep += 1\n",
    "            training_rewards.append(rewards)\n",
    "            training_loss.append(np.mean(update_loss))\n",
    "            update_loss = []\n",
    "            mean_rewards = np.mean(training_rewards[-WINDOW:])\n",
    "            mean_training_rewards.append(mean_rewards)\n",
    "            print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                ep, mean_rewards), end=\"\")\n",
    "\n",
    "            if ep >= MAX_EPISODES:\n",
    "                training = False\n",
    "                print('\\nEpisode limit reached.')\n",
    "                break\n",
    "            if mean_rewards >= REWARD_THRESHOLD:\n",
    "                training = False\n",
    "                print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                    ep))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
