{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free Reinforcement Learning with policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will program a model-free reinforcement learning algorithm policy gradient. The tasks will be:\n",
    "\n",
    "    Explore the environment\n",
    "    Design the policy\n",
    "    Compute action selection\n",
    "    Estimate the expected return\n",
    "    Backpropagate through the policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all neccesary modules\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the environment\n",
    "gamma = 0.99 #discoutn factor\n",
    "render = True #render the environment\n",
    "log_interval = 10\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "for i_episode in range(10):\n",
    "    state, ep_reward = env.reset(), 0\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "env.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get familiar with the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the action and state space. (discrete or continuous, how many actions (states) are available, what do the actions (states) describe?)\n",
    "\n",
    "State space:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to 'nn.py' to implement the policy (Task 1.1 & Task 1.2). Contrary to Assignment1, you are defining the neural network policy as a class. In PyTorch, the minimal setup of a neural network in a class requieres at least two functions, the initialization and the forward function. During initialization you have to initialize all instances of layers, e.g. fully connected layer, convolutional layer ... . Additional variables, e.g. lists, constants, ..., are also set during initialization. In the forward function you are connecting these instances to a neural network. \n",
    "\n",
    "    Policy architecture:\n",
    "    fully connected layer (#states -> 128)\n",
    "    dropout layer (p=0.6)\n",
    "    ReLU activation\n",
    "    fully connected layer (128 -> #actions)\n",
    "    softmax activation\n",
    "    \n",
    "Additionally, the policy needs a list for logartihmic probabilities of all actions and all rewards of a single trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # Task 1.1:\n",
    "        # Initialize all necessary layers \n",
    "        # Use the layers from torch.nn\n",
    "        # Don't forget to initialize two lists, one for the log probs and one for the rewards\n",
    "\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Task 1.2:\n",
    "        # Now, connect all layers to a neural network and return the probabilities for all actions\n",
    "        # Use activation functions from torch.nn.functional\n",
    "\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "        pass  # to be replaced by your code\n",
    "        ######################################## END OF YOUR CODE ##########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, initialize an instance of the neural network policy. To update the weights, use an Adam optimizer (torch.optim) with a learning rate of lr=1e-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.3:\n",
    "# Initialize the policy and the optimizer\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "pass  # to be replaced by your code\n",
    "\n",
    "######################################## END OF YOUR CODE ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample an action from the probability distribution over actions of the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to sample an action from the probability distribution computed by the policy. First, convert the state to a torch tensor. Then, compute the action probabilities and define a categorical distribution over the action probabilities. Last, sample an action from this probability and save the log probability of this action in the corresponding list of the policy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1:\n",
    "# Define the function for sampling an action\n",
    "def sample_action(state):\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "   \n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    return action.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the exptected returns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, you need to compute the estimated return for each state-action pair with respect to the corresponding trajectory. In order to compute the estimate of the expectation over the remaining discounted rewards it is quite reasonable to iterate backwards through the reward list stored in the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.1: \n",
    "# Define a running variable for the current rewards and a list to store the expected return for each state.\n",
    "# Iterate backwards through all rewards stored in the corresponding list of the policy and compute the discounted \n",
    "# expected return for all remaining states of the current trajectory. (gamma is already defined)\n",
    "# Note, when iterating backwards, appending the expected return is not a smart move!!!\n",
    "def estimate_return():\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "   \n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    return returns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the loss and update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the estimates for the exptected discounted, you need to update the weights of the policy. Remember that the only difference between the gradient in supervised learning and the gradient of the RL objective in policy gradient is the weighting factor based on the expected return (see the slides of the lecture). To compute the loss of a trajectory, sum over the negative log-likelihood of the chosen action multiplied by the expected return of the current state for all states of the trajectory. Then, use the backwards-function to compute the gradient and update the weights using the optimizer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.1:\n",
    "# Define a list for the losses of each state in a trajectory\n",
    "# Compute the individual loss for each state based on the negative log likelihood of the action and the expected\n",
    "# return of the current state (you have to iterate over the complete trajectory)\n",
    "# Prepare the optimizer for an update step\n",
    "# Compute the overall loss of the policy (sum)\n",
    "# Compute the gradient\n",
    "# Update the weights of the network\n",
    "# Don't forget to delete the stored rewards and the action probs in the corresponding lists of the policy\n",
    "def update_weights(returns):\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "    \n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to combine everything. Update the policy with 1000 trajectory samples. The openai gym environments return a terminal flag which you can use to notice a terminal state. Remember policy gradient is on-policy meaning that it is necessary to generate new samples after every single update step. Visualize the evolution of the rewards over time in a plot. Use an exponential weighted moving average over the rewards to get a smooth graph (delta = 0.05). Additionaly, illustrate the cumulative reward of every trajectory in the same plot. Print out after every ten trajectories the trajectory counter, the averaged reward and the last reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6.1:\n",
    "# Generate 1000 trajectories with a maximum length of 10000. \n",
    "running_reward = 10\n",
    "render = False\n",
    "\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "\n",
    "pass  # to be replaced by your code\n",
    "\n",
    "######################################## END OF YOUR CODE ##########################################        \n",
    "plt.plot(true_reward)\n",
    "plt.plot(averaged_reward)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render your learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't have to do anything here.\n",
    "env = gym.make('CartPole-v1')\n",
    "render = True\n",
    "for i_episode in range(10):\n",
    "    state, ep_reward = env.reset(), 0\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = sample_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "env.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
