{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-free Reinforcement Learning with policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will program a model-free reinforcement learning algorithm policy gradient. The tasks will be:\n",
    "\n",
    "    Explore the environment\n",
    "    Design the policy\n",
    "    Compute action selection\n",
    "    Estimate the expected return\n",
    "    Backpropagate through the policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all neccesary modules\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the environment\n",
    "gamma = 0.99 #discoutn factor\n",
    "render = True #render the environment\n",
    "log_interval = 10\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "for i_episode in range(10):\n",
    "    state, ep_reward = env.reset(), 0\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "env.close()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get familiar with the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the action and state space. (discrete or continuous, how many actions (states) are available, what do the actions (states) describe?)\n",
    "\n",
    "State space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state space describes the set of all possible states that the agent can be in. If the agent conducts an action it will move from one state to another state inside the state space. Discrete spaces contain a finite and discrete number of states, while continuous states contain an infinite number of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space is the set of all possible actions that an agent can conduct in his environment. Given a specific state, the agent can conduct certain actions, which are modelled as a map. This map is called policy function and it provides the probability that an agent will conduct a certain action when in a specific state. Again, a discrete action space contains a finite and discrete number of actions while a continuous action state can contain any number of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to 'nn.py' to implement the policy (Task 1.1 & Task 1.2). Contrary to Assignment1, you are defining the neural network policy as a class. In PyTorch, the minimal setup of a neural network in a class requieres at least two functions, the initialization and the forward function. During initialization you have to initialize all instances of layers, e.g. fully connected layer, convolutional layer ... . Additional variables, e.g. lists, constants, ..., are also set during initialization. In the forward function you are connecting these instances to a neural network. \n",
    "\n",
    "    Policy architecture:\n",
    "    fully connected layer (#states -> 128)\n",
    "    dropout layer (p=0.6)\n",
    "    ReLU activation\n",
    "    fully connected layer (128 -> #actions)\n",
    "    softmax activation\n",
    "    \n",
    "Additionally, the policy needs a list for logartihmic probabilities of all actions and all rewards of a single trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # Task 1.1:\n",
    "        # Initialize all necessary layers \n",
    "        # Use the layers from torch.nn\n",
    "        # Don't forget to initialize two lists, one for the log probs and one for the rewards\n",
    "\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "        self.fc1 = nn.Linear(num_of_states, 128)\n",
    "        self.dropout1 =  nn.Dropout(p=0.6)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_of_actions)\n",
    "        self.softmax2 = nn.SoftMax\n",
    "        self.log_probs = []\n",
    "        # do the same for the rewards\n",
    "        pass  # to be replaced by your code\n",
    "\n",
    "        ######################################## END OF YOUR CODE ##########################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Task 1.2:\n",
    "        # Now, connect all layers to a neural network and return the probabilities for all actions\n",
    "        # Use activation functions from torch.nn.functional\n",
    "\n",
    "        ######################################## START OF YOUR CODE ########################################\n",
    "        #define how our NNs are connected\n",
    "        output = self.fc1(x) # x is the input data\n",
    "        output = self.dropout1(output)\n",
    "        output = F.ReLU(output) # applying the greedy activation function to the output\n",
    "        pass  # to be replaced by your code\n",
    "        ######################################## END OF YOUR CODE ##########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, initialize an instance of the neural network policy. To update the weights, use an Adam optimizer (torch.optim) with a learning rate of lr=1e-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.3:\n",
    "# Initialize the policy and the optimizer\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "policy = Policy()\n",
    "# we need an optimizer\n",
    "optimizer = optim.Adam( assign a few parameters, find them in the API)\n",
    "\n",
    "\n",
    "pass  # to be replaced by your code\n",
    "\n",
    "######################################## END OF YOUR CODE ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample an action from the probability distribution over actions of the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to sample an action from the probability distribution computed by the policy. First, convert the state to a torch tensor. Then, compute the action probabilities and define a categorical distribution over the action probabilities. Last, sample an action from this probability and save the log probability of this action in the corresponding list of the policy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1:\n",
    "# Define the function for sampling an action\n",
    "def sample_action(state):\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "   # transform this into a torch tensor, also cast it to a 32bit float tensor and use unsqueeze\n",
    "    state = torch.from_numpy(state).float().unsqueeze()\n",
    "    # put in into the policy\n",
    "    prob = policy(state)\n",
    "    # define categorical destribution over all actions in prob (distribution also in the API)\n",
    "    # now we have to sample from distribution, command is in the documentation\n",
    "    sample = distribution.log_prob(action) # this line is not 100% correct!!! the command is called something like log_prob(action)\n",
    "    # now append to the log_probs list of the policy\n",
    "    policy.log_probs.append(sample)\n",
    "    \n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    return action.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the exptected returns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, you need to compute the estimated return for each state-action pair with respect to the corresponding trajectory. In order to compute the estimate of the expectation over the remaining discounted rewards it is quite reasonable to iterate backwards through the reward list stored in the policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.1: \n",
    "# Define a running variable for the current rewards and a list to store the expected return for each state.\n",
    "# Iterate backwards through all rewards stored in the corresponding list of the policy and compute the discounted \n",
    "# expected return for all remaining states of the current trajectory. (gamma is already defined)\n",
    "# Note, when iterating backwards, appending the expected return is not a smart move!!!\n",
    "def estimate_return():\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "    # we define a backwards loop which iterates over the ... trajectory\n",
    "    R = 0 # will be updated incrementally\n",
    "    rewardlist = [] # list of rewared for every state\n",
    "    \n",
    "    for r in policy.rewards[::-1]: # we iterate backwards in our trajectory\n",
    "        # compute the discounted reward. R = running variable over all states (the reward so far)\n",
    "        # r = reward which comes from our list\n",
    "        R = r + gamma * R  # discounted\n",
    "        # add R to rewardlist\n",
    "        \n",
    "    # also normalize data to increase neural network performance\n",
    "    # normalize = returns.mean()/(returns.std()) + eps \n",
    "    # eps is a stabilizing factor \n",
    "    \n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################\n",
    "    return returns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the loss and update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the estimates for the exptected discounted, you need to update the weights of the policy. Remember that the only difference between the gradient in supervised learning and the gradient of the RL objective in policy gradient is the weighting factor based on the expected return (see the slides of the lecture). To compute the loss of a trajectory, sum over the negative log-likelihood of the chosen action multiplied by the expected return of the current state for all states of the trajectory. Then, use the backwards-function to compute the gradient and update the weights using the optimizer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.1:\n",
    "# Define a list for the losses of each state in a trajectory\n",
    "# Compute the individual loss for each state based on the negative log likelihood of the action and the expected\n",
    "# return of the current state (you have to iterate over the complete trajectory)\n",
    "# Prepare the optimizer for an update step\n",
    "# Compute the overall loss of the policy (sum)\n",
    "# Compute the gradient\n",
    "# Update the weights of the network\n",
    "# Don't forget to delete the stored rewards and the action probs in the corresponding lists of the policy\n",
    "def update_weights(returns):\n",
    "    ######################################## START OF YOUR CODE ########################################\n",
    "    losses = []\n",
    "    \n",
    "    for log_probs, R in zip(policy.log_probs, returns): # we can use multiple lists at a time by using zip function\n",
    "        #compute the nll of the return: just multiply by expected return and add to list\n",
    "        # something like this: append(-log_probs*expected_return)\n",
    "    # now zero out gradient in optimizer (there is a command for this)\n",
    "    # now sum up the list\n",
    "    torch.cat(policy_Loss).sum() #concatination of all entries in our list. maybe policy_Loss is losses\n",
    "    # now backward command\n",
    "    # and optimizer takes a step\n",
    "    # delete what is in policy.log_probs\n",
    "    # delete what is in policy.rewards so we can start from the beginning again\n",
    "    \n",
    "    pass  # to be replaced by your code\n",
    "\n",
    "    ######################################## END OF YOUR CODE ##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to combine everything. Update the policy with 1000 trajectory samples. The openai gym environments return a terminal flag which you can use to notice a terminal state. Remember policy gradient is on-policy meaning that it is necessary to generate new samples after every single update step. Visualize the evolution of the rewards over time in a plot. Use an exponential weighted moving average over the rewards to get a smooth graph (delta = 0.05). Additionaly, illustrate the cumulative reward of every trajectory in the same plot. Print out after every ten trajectories the trajectory counter, the averaged reward and the last reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6.1:\n",
    "# Generate 1000 trajectories with a maximum length of 10000. \n",
    "running_reward = 10\n",
    "render = False\n",
    "\n",
    "######################################## START OF YOUR CODE ########################################\n",
    "# just follow what is explained for this\n",
    "pass  # to be replaced by your code\n",
    "\n",
    "# from Kai Lagemann's email to stop the learning process if the exponentially weighted moving average \n",
    "# is higher than the threshold\n",
    "if i_episode % log_interval == 0:\n",
    "    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "              i_episode, ep_reward, running_reward))\n",
    "if running_reward > env.spec.reward_threshold:\n",
    "    print(\"Solved! Running reward is now {} and \"\n",
    "              \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "    break\n",
    "######################################## END OF YOUR CODE ##########################################        \n",
    "plt.plot(true_reward)\n",
    "plt.plot(averaged_reward)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render your learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't have to do anything here.\n",
    "env = gym.make('CartPole-v1')\n",
    "render = True\n",
    "for i_episode in range(10):\n",
    "    state, ep_reward = env.reset(), 0\n",
    "    for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "        action = sample_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "env.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
